<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Reinforcement Learning - Dynamic programming</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="http://localhost:4000/Revision/">Revision</a></h1>
    <h2 id="back-to-value-based-methods"><a href="/Revision/reinforcement_learning_value_based_methods.html">Back to Value based methods</a></h2>

<p><br /></p>
<h1 id="introduction">Introduction</h1>
<p>Dynamic programming (DP) is used for finite environment.
DP is a model based learning method based on dynamic programming and the Bellman’s equations.</p>

<p>In this setup (environment with finite number of states and actions where the model is known ie with known rewards \(r\) and transitions probabilities \(p(s',r \; \vert \; s,a)\)) a simple application of Bellman’s equation is sufficient to estimate the state and action value functions.</p>

<p>This setup doesn’t require to leverage interactions between the agent and the environment.</p>

<p><br /></p>
<h1 id="policy-evaluation">Policy evaluation</h1>
<p>The policy evaluation algorithm is used to estimate the value function associated with the policy.</p>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/policy_evaluation.png" width="75%" height="75%" />
</div>
<p><br /></p>

<p>One can set a maximum of iteration to avoid the algorithm to never stop.</p>

<p><br /></p>
<h2 id="estimation-of-action-value-function">Estimation of action value function</h2>
<p>Once the state value function has been estimated using the policy evaluation algorithm, one can easily estimate the action value function as follow:</p>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/estimation_action_value.png" width="75%" height="75%" />
</div>
<p><br /></p>

<p><br /></p>
<h2 id="policy-improvement">Policy improvement</h2>
<p>Given a policy \(\pi\) and its state value function \(v_\pi\), one can apply a policy improvement algorithm to obtain a new policy \(\pi'\) such that \(\pi' \geq \pi\) (ie \(v_{\pi'}(s) \geq v_{\pi}(s) \forall s \in \mathcal{S}\)):</p>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/policy_improvement.png" width="75%" height="75%" />
</div>
<p><br /></p>

<p>For each state, the algorithm will estimate the best action to take using its state value function (or directly using its action value function).
It will update the policy by associating the action \(a\) that maximises reward to the state \(s\).</p>

<p>The precedent policy \(\pi\), used to evaluate the state value function, used an equally weight probability for all action of a state in order to estimate the state value function (see <a href="#policy-evaluation">Policy evaluation</a>).</p>

<p><br /></p>
<h1 id="policy-iteration">Policy iteration</h1>
<p>Using successively policy estimation and policy improvement, one can start with a simple (equiprobable) policy and improve it step by step:</p>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/policy_iteration1.png" width="75%" height="75%" />
</div>
<p>The algorithm alternates between the two methods: ‘Policy evaluation’ and ‘Policy improvement’.</p>

<p><br /></p>

<p>Here is a schematic view of the algorithm:</p>
<div style="text-align: center">
<img src="assets/images/policy_iteration2.png" width="30%" height="30%" />
</div>

<p><br /></p>
<h1 id="limitations">Limitations</h1>
<ul>
  <li>Finite number of states,</li>
  <li>Finite number of actions,</li>
  <li>Need knowledge of the underlying model of the environment,</li>
  <li>Deterministic policy.</li>
</ul>

<p><br /></p>
<h1 id="resources">Resources</h1>
<p>See:</p>
<ul>
  <li>UDRLN cheat sheet part 4,</li>
  <li>UDRLN video 2.1.5.</li>
</ul>

  </body>
</html>
