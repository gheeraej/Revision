<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Mathematics - Functional Analysis</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="http://localhost:4000/Revision/">Revision</a></h1>
    <h2 id="back-to-general-mathematics"><a href="/Revision/mathematics.html">Back to General Mathematics</a></h2>

<p><br /></p>
<h1 id="taylor-expansion">Taylor expansion</h1>
<p>Taylor series of a function is an infinite sum of terms that are expressed in terms of the function’s derivatives at a single point.
The Taylor series of a real or complex-valued function \(f(x)\) that is infinitely differentiable at a real or complex number \(a\) is the power series:</p>

\[f(a)+\frac{f^{'}(a)}{1!}(x-a)+\frac{f^{(2)}(a)}{2!}(x-a)^2+\frac{f^{(3)}(a)}{3!}(x-a)^3+ ...\]

<p>See: <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor series on Wikipedia</a>.</p>

<p><br /></p>
<h3 id="taylors-theorem">Taylor’s theorem</h3>
<p>Taylor theorem is used to approximate a \(k\)-times differentiable function \(f(x)\) around a given point \(x\) by a polynomial of degree \(k\). For a smooth function, the Taylor polynomial is the Taylor series of the function considered for the \(k\)-th first elements.</p>

<p>The first-order Taylor polynomial is the linear approximation of the function, and the second-order Taylor polynomial is the quadratic approximation.</p>

<p>Let \(f(x)\) be a \(k\)-times differentiable function on point \(a\), then for an \(x\) in the neighbourhood of \(a\):</p>

\[\begin{eqnarray}
f(x) &amp;&amp;= f(a)+\frac{f^{'}(a)}{1!}(x-a)+\frac{f^{(2)}(a)}{2!}(x-a)^2 + ... + \frac{f^{(k)}(a)}{k!}(x-a)^k + o((x-a)^k) \\
&amp;&amp;= \sum_{i=0}^{k}\left[\frac{f^{(i)}(a)}{i!}(x-a)^i\right] + o((x-a)^k)
\end{eqnarray}\]

<p>It can be rewrite using \(h\) close to \(0\) as:</p>

\[\begin{eqnarray}
f(a+h) &amp;&amp;= f(a)+\frac{f^{'}(a)}{1!}h+\frac{f^{(2)}(a)}{2!}h^2 + ... + \frac{f^{(k)}(a)}{k!}h^k + o(h^k) \\
&amp;&amp;= \sum_{i=0}^{k}\left[\frac{f^{(i)}(a)}{i!}h^i\right] + o(h^k)
\end{eqnarray}\]

<p>If \(f(x)\) is \(k\)-times differentiable at point \(0\) then:</p>

\[\begin{eqnarray}
f(x) &amp;&amp;= f(0)+\frac{f^{'}(0)}{1!}x+\frac{f^{(2)}(0)}{2!}x^2 + ... + \frac{f^{(k)}(0)}{k!}x^k + o(x^k) \\
&amp;&amp;= \sum_{i=0}^{k}\left[\frac{f^{(i)}(0)}{i!}x^i\right] + o(x^k)
\end{eqnarray}\]

<p><br /></p>
<h2 id="resources">Resources</h2>
<p>See:</p>
<ul>
  <li>Taylor’s theorem on Wikipedia <a href="https://en.wikipedia.org/wiki/Taylor%27s_theorem">in english</a> and <a href="https://fr.wikipedia.org/wiki/Théorème_de_Taylor">in french</a>.</li>
</ul>

<p><br /><br /></p>
<h1 id="jensens-inequality">Jensen’s inequality</h1>
<p>Jensen’s inequality states that the convex transformation of a mean is less than or equal to the mean applied after convex transformation.</p>

<p><br /></p>
<h2 id="for-2-points">For 2 points</h2>
<p>Jensen’s inequality states that for a convex function \(f\), the secant line joining two points of the function graph is above the function graph.</p>

<p>From a mathematical point of view, the function \(f\) applied to the weighted mean of two points \(a\) and \(b\) \(f(ta + (1-t)b)\) is smaller or equal than the weighted mean of the function applied to these two points \(a\) and \(b\) \(tf(a) + (1-t)f(b)\):</p>

\[f(ta + (1-t)b) \leq tf(a) + (1-t)f(b)\]

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/jensen.png" width="40%" height="40%" />
</div>
<p><br /></p>

<h4 id="for-concav-function">For concav function</h4>
<p>Jensen’s inequality states that for a concav function \(f\), the secant line joining two points of the function graph is below the function graph.</p>

<p>From a mathematical point of view, the function \(f\) applied to the weighted mean of two points \(a\) and \(b\) \(f(ta + (1-t)b)\) is greater or equal than the weighted mean of the function applied to these two points \(a\) and \(b\) \(tf(a) + (1-t)f(b)\):</p>

\[f(ta + (1-t)b) \geq tf(a) + (1-t)f(b)\]

<p><br /></p>
<h3 id="probabilistic-jensens-inequality">Probabilistic Jensen’s inequality</h3>
<p>It can be directly applied to expected value of a convex function \(\phi\):</p>

\[\varphi(\mathbb{E}[X]) \leq \mathbb{E}[\varphi(X)]\]

<p>For a function \(\phi\) concave we have:</p>

\[\varphi(\mathbb{E}[X]) \geq \mathbb{E}[\varphi(X)]\]

<p><br /></p>
<h2 id="finite-form">Finite form</h2>
<p>The finite form of Jensen’s inequality is, for a convex function \(f\):</p>

\[\varphi \left(\frac {\sum a_{i}x_{i}} {\sum a_{i}}\right) \leq \frac {\sum (a_{i}\varphi (x_{i}))}{\sum a_{i}}\]

<p>For a concave function we have:</p>

\[\varphi \left(\frac {\sum a_{i}x_{i}} {\sum a_{i}}\right) \geq \frac {\sum (a_{i}\varphi (x_{i}))}{\sum a_{i}}\]

<p><br /></p>
<h2 id="resources-1">Resources:</h2>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Wikipedia page of Jensen’s inequality</a>.</li>
</ul>

  </body>
</html>
