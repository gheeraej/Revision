<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Metrics</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="http://localhost:4000/Revision/">Revision</a></h1>
    <h2 id="back-to-machine-learning"><a href="/Revision/machine_learning.html">Back to Machine Learning</a></h2>

<p><br /></p>
<h1 id="supervised-metrics">Supervised Metrics</h1>

<p><br /></p>
<h2 id="regression">Regression</h2>
<h3 id="r2-coefficient-of-determination">\(R^2\): Coefficient of determination</h3>
<p>In statistics, the coefficient of determination, denoted \(R^2\) or \(r^2\) is the proportion of the variation in the observed variable (y) that is predictable from the explanatory variables (X). It is a performance metric for linear regression.</p>

<p><br /></p>
<h4 id="formula">Formula</h4>
<p>For given predictions \(\hat{y}_i\) and true labels \(y_i\), the \(R^2\) is:</p>

\[R^2=1-\frac{SS_{Residual}}{SS_{Total}}=\frac{SS_{Explained}}{SS_{Total}}\]

<p>Where:</p>
<ul>
  <li>\(SS_{Total}=\sum_{i=1}^n\left(y_i - \bar{y}\right)^2\) represents the total variance of \(y\),</li>
  <li>\(SS_{Explained}=\sum_{i=1}^n\left(\hat{y}_i - \bar{y}\right)^2\) represents the variance explained by the estimators \(\hat{y}\),</li>
  <li>\(SS_{Residual}=\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2\) represents the remaining variance, unexplained by the model.</li>
</ul>

<p>And law of total variance: \(SS_{Total}=SS_{Explained}+SS_{Residual}\).</p>

<p>The following graphics from Wikipedia shows a visual interpretation:</p>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/r2.png" width="30%" height="30%" />
</div>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/r22.png" width="10%" height="10%" />
</div>

<p><br /></p>
<h4 id="resources">Resources</h4>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Coefficient of determination on Wikipedia</a></li>
</ul>

<p><br /></p>
<h3 id="root-mean-square-error">Root Mean Square Error</h3>
<p>For given predictions \(\hat{y}_i\) and true labels \(y_i\), the RMSE (or root mean square deviation - RMSD) loss is:</p>

\[RMSE = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}}\]

<p><br /></p>
<h4 id="resources-1">Resources</h4>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">Root mean square error on Wikipedia</a>.</li>
</ul>

<p><br /></p>
<h3 id="mean-absolute-error">Mean Absolute Error</h3>
<p>For given predictions \(\hat{y}_i\) and true labels \(y_i\), the MAE is:</p>

\[MAE = \frac{\sum_{i=1}^n \vert y_i - \hat{y}_i \vert}{n}\]

<p><br /></p>
<h4 id="resources-2">Resources</h4>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean absolute error on Wikipedia</a>.</li>
</ul>

<p><br /><br /></p>
<h2 id="classification">Classification</h2>
<h3 id="confusion-matrix--precision--recall--specificity--f1-score">Confusion Matrix / Precision / Recall / Specificity / F1-Score</h3>
<p>Here is a representation of a confusion matrix:</p>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/confusion_matrix.png" width="40%" height="40%" />
</div>
<p><br /></p>

<p>Where:</p>
<ul>
  <li>True positive (TP): a test result that correctly indicates the presence of a condition or characteristic</li>
  <li>True negative (TN): a test result that correctly indicates the absence of a condition or characteristic</li>
  <li>False positive (FP or Type I Error): a test result which wrongly indicates that a particular condition or attribute is present</li>
  <li>False negative (FN or Type II Error): a test result which wrongly indicates that a particular condition or attribute is absent</li>
</ul>

<p><br /></p>
<h4 id="precision">Precision</h4>
<p>Precision measures how the model is accurate for the positive predictions:</p>

\[Precision=\frac{TP}{TP+FP}\]

<p><br /></p>
<h4 id="recall-or-sensitivity-hit-rate-or-true-positive-rate">Recall (or Sensitivity, Hit Rate or True Positive Rate)</h4>
<p>Recall measures the percentage of the positive population that was detected positive:</p>

\[Recall=\frac{TP}{TP+FN}=\frac{TP}{P}\]

<p><br /></p>
<h4 id="false-positive-rate">False Positive Rate</h4>
<p>False Positive Rate measures the percentage of the negative population that was detected positive:</p>

\[Recall=\frac{FP}{FP+TN}=\frac{TP}{P}\]

<p><br /></p>
<h4 id="specificity-or-selectivity-or-true-negative-rate">Specificity (or Selectivity or True Negative Rate)</h4>
<p>Specificity measures the percentage of the negative population that was detected negative:</p>

\[Specificity=\frac{TN}{TN+FP}=\frac{TN}{N}\]

<p><br /></p>
<h4 id="f1-score">F1-Score</h4>
<p>\(F_1\)-Score is an harmonic mean of precision and recall.
For two number \(X_1\) and \(X_2\), the harmonic mean is:</p>

\[H(X_1, X_2)=2 \times \frac{X_1 X_2}{X_1 + X_2}\]

<p>So the \(F_1\)-Score is:</p>

\[F_1=2 \times \frac{Precision \times Recall}{Precision + Recall}\]

<p><br /></p>
<h3 id="accuracy">Accuracy</h3>
<p>Accuracy which is a natural metric is just the percentage of well predict samples:</p>

\[Accuracy = \frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{n}\]

<p>Where:</p>
<ul>
  <li>\(n\) is the total number of samples.</li>
</ul>

<p><br /></p>
<h3 id="roc-curve">ROC Curve</h3>
<p>ROC (for Receiver operating characteristic) Curve is a curve created where each point corresponds to the results obtained for a given threshold.
It plots, for every thresholds, the True Positive Rate against the False Positive Rate:</p>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/roc_curve.svg" width="25%" height="25%" />
</div>

<p>For a threshold of 0, the TPR would be 1 (every element of the positive population detected positive) and the FPR would also be 1 (every element of the negative population detected positive).</p>

<p>For a threshold of 1, the TPR would be 0 (every element of the positive population detected negative) and the FPR would also be 0 (no element of the negative population detected positive).</p>

<p>Other threshold are in between. A perfect classifier would have a TPR of 1 every element of the positive population detected positive) and a FPR of 0 (no element of the negative population detected positive).</p>

<p><br /></p>
<h3 id="auc">AUC</h3>
<p>AUC for Area Under Curve is the area under the ROC curve or its integral.
When using normalized units, AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.</p>

<p>AUC is related to the Mannâ€“Whitney U and to the Gini coefficient (not the Gini impurity).</p>

<p>See <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">the paragraph dedicated to AUC on the Wikipedia page for ROC Curve</a>.</p>

<h3 id="resources-3">Resources</h3>
<p>For all of the classification metrics see:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Wikipedia page on ROC Curve (with references to other classification metrics)</a>.</li>
</ul>

<p><br /><br /></p>
<h1 id="unsupervised-metrics">Unsupervised Metrics</h1>
<p>Most of unsupervised metrics (without labels) are based on the variance intra clusters and the variance inter cluster.</p>

<p><br /></p>
<h2 id="silhouette-coefficient">Silhouette coefficient</h2>
<p>Silhouette coefficient is a clustering metric defined for a single cluster as:</p>

\[s=\frac{b-a}{\max(a,b)}\]

<p>Where:</p>
<ul>
  <li>a is the mean distance between a sample and all other points in the same cluster (or class),</li>
  <li>b is the mean distance between a sample and all other points in the next nearest cluster.</li>
</ul>

<p>For a set of cluster in is then:</p>

\[s=\frac{1}{n_{clusters}}\sum_{i=1}^{n_{clusters}}\frac{b_i-a_i}{\max(a_i,b_i)}\]

<p>If a cluster is very dense and far from its nearest neighbours then is silhouette coefficient will be high.
On the contrary a sparse cluster not isolated from its neighbours will have a low silhouette coefficient.</p>

<p><br /></p>
<h4 id="pros-and-cons">Pros and Cons</h4>
<h5 id="pros">Pros</h5>
<ul>
  <li>The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters,</li>
  <li>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</li>
</ul>

<h5 id="cons">Cons</h5>
<ul>
  <li>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</li>
</ul>

<p><br /></p>
<h2 id="calinski-harabasz-index">Calinski-Harabasz Index</h2>
<p>Calinski-Harabasz Index is a clustering metric defined, for a dataset \(E\) as:</p>

\[s=\frac{B}{W}\frac{n_E-k}{k-1}\]

<p>Where \(B_k\) is the between group dispersion measure and \(W_k\) is the within-cluster dispersion measure defined by:</p>
<ul>
  <li>\(B=\sum_{q=1}^{k}\sum_{x \in C_q}(x-c_q)(x-c_q)^t\),</li>
  <li>\(W=\sum_{q=1}^{k}n_q(c_q-c_E)(c_q-c_E)^t\).</li>
</ul>

<p>With:</p>
<ul>
  <li>\(C_q\) the set of points in cluster \(q\),</li>
  <li>\(c_q\) the center of cluster \(q\),</li>
  <li>\(c_E\) the center of E (center of the dataset),</li>
  <li>\(n_E\) the size of the dataset (number of data points),</li>
  <li>\(n_q\) the number of data points in the cluster \(q\).</li>
</ul>

<p>The Calinski-Harabasz index is thus the ratio of the sum of between-clusters dispersion (variance inter) and of within-cluster dispersion (variance intra) for all clusters (where dispersion is defined as the sum of distances squared):
The \(\frac{n_E-k}{k-1}\) is a penalty on the number of cluster.</p>

<p>For Calinski-Harabasz Index, an higher score is better.</p>

<p><br /></p>
<h4 id="pros-and-cons-1">Pros and Cons</h4>
<h5 id="pros-1">Pros</h5>
<ul>
  <li>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster,</li>
  <li>The score is fast to compute.</li>
</ul>

<h5 id="cons-1">Cons</h5>
<ul>
  <li>The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</li>
</ul>

<p><br /></p>
<h2 id="davies-bouldin-index">Davies-Bouldin Index</h2>
<p>Davies-Bouldin Index is a clustering metric defined, for a dataset \(E\) as:</p>

\[DB=\frac{1}{k}\sum_{i=1}^k \max_{i \ne j}R_{ij}\]

<p>Where:</p>
<ul>
  <li>\(R_{ij}=\frac{s_i + s_j}{d_{ij}}\),</li>
</ul>

<p>With:</p>
<ul>
  <li>\(s_i\) the average distance between each point of cluster and the centroid of that cluster â€“ also know as cluster diameter,</li>
  <li>\(d_{ij}\) the distance between cluster centroids \(i\) and \(j\).</li>
</ul>

<p>By taking, for each \(i\), the maximum score \(R_{ij}\) the Davies-Bouldin Index just looks at the score for cluster \(i\) compare to its closest neighbour (similar to Silhouette score). It will compare this distance to the sum of the average distance in cluster \(i\) and in cluster \(j\).</p>

<p>Zero is the lowest possible score. Values closer to zero indicate a better partition.</p>

<p><br /></p>
<h4 id="pros-and-cons-2">Pros and Cons</h4>
<h5 id="pros-2">Pros</h5>
<ul>
  <li>The computation of Davies-Bouldin is simpler than that of Silhouette scores,</li>
  <li>The index is solely based on quantities and features inherent to the dataset as its computation only uses point-wise distances.</li>
</ul>

<h5 id="cons-2">Cons</h5>
<ul>
  <li>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN,</li>
  <li>The usage of centroid distance limits the distance metric to Euclidean space.</li>
</ul>

<p><br /></p>
<h2 id="resources-4">Resources</h2>
<p>See:</p>
<ul>
  <li><a href="https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation">Scikit-Learn page on Unsupervised metrics</a>.</li>
</ul>

<p><br /></p>
<h1 id="other-metrics">Other metrics</h1>
<p>See:</p>
<ul>
  <li><a href="https://cs230.stanford.edu/section/8/">CS230 section 8 for other metrics</a>.</li>
</ul>

  </body>
</html>
