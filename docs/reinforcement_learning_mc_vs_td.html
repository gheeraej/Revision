<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>MC vs TD</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="http://localhost:4000/Revision/">Revision</a></h1>
    <h2 id="back-to-value-based-methods"><a href="/Revision/reinforcement_learning_value_based_methods.html">Back to Value based methods</a></h2>

<p><br /></p>
<h1 id="comparison-of-mc-and-td-methods">Comparison of MC and TD methods</h1>
<p><br /></p>
<div style="text-align: center">
<img src="assets/images/MC_TD.png" width="40%" height="40%" />
</div>
<p>MC method takes the value \(G_t\) as alternative estimate. \(G_t\) is the return computed using the future states of the episode. TD methods makes its alternative estimate based on the observed return and the next state/action estimate. Hence TD does not require the whole episode to make its guess of alternative estimate.</p>

<p><br /></p>
<h2 id="biais-and-variance">Biais and variance</h2>
<p>Monte-Carlo method returns an unbiaised estimator but with high variance, as for each episode it cumulates different random values (the rewards are random as well as the next states).</p>

<p>Temporal difference method is biaised as the return of the next state is estimated (\(G_{t+1}\)) but it has low variance (as it does not cumulate different random values).</p>

<p>This picture highlights the high number of random values in MC method and the estimate \(G_{t+1}\) in TD method:
<br /></p>
<div style="text-align: center">
<img src="assets/images/RL_biais_variance.png" width="40%" height="40%" />
</div>

<p><br /></p>
<h1 id="resources">Resources</h1>
<p>See:</p>
<ul>
  <li>UDRLN videos 3.4.4.</li>
</ul>

  </body>
</html>
