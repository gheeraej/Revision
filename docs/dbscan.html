<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>DBSCAN</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="/Revision/">Revision</a></h1>
    <h2 id="back-to-machine-learning"><a href="/Revision/machine_learning.html">Back to Machine Learning</a></h2>

<p><br /></p>
<h1 id="definition">Definition</h1>
<p>DBSCAN (for Density-based spatial clustering of applications with noise) is a density based clustering method that computes for each point in the dataset its neighbours based on a given distance \(\varepsilon\).</p>

<p>\(\varepsilon\) is an hyperparameter of DBSCAN and the other hyperparameter of the model is \(min_{points}\).</p>

<p><br /></p>
<h1 id="notations">Notations</h1>
<ul>
  <li>Two data points \(p\) and \(q\) are considered neighbours if \(D(p, q) \lt \varepsilon\) where \(D\) is a given distance measure (euclidian distance for example),</li>
  <li>A point \(p\) is a core point if at least \(min_{points}\) neighbours, ie \(min_{points}\) points which are within a distance \(\varepsilon\) of it (including \(p\)),</li>
  <li>A point \(q\) is directly reachable from \(p\) if point \(q\) is within distance \(\varepsilon\) from core point \(p\). Points are only said to be directly reachable from core points,</li>
  <li>A point \(q\) is reachable from \(p\) if there is a path \(p_1, \ldots, p_n\) with \(p_1 = p\) and \(p_n = q\), where each \(p_{i+1}\) is directly reachable from \(p_i\). Note that this implies that the initial point and all points on the path must be core points, with the possible exception of \(q\).</li>
</ul>

<p>A cluster is thus a set of close core points with distance less than \(\varepsilon\) and non core sample but having a distance less than \(\varepsilon\) to at least one core point.
Data points with distance more than \(\varepsilon\) to every core points are considered outliers.</p>

<p><br /></p>
<h1 id="visual-representation">Visual representation</h1>
<p><br /></p>
<div style="text-align: center">
<img src="assets/images/dbscan.png" width="30%" height="30%" />
</div>
<p>In this diagram,  \(min_{points}=4\). Point \(A\) and the other red points are core points, because the area surrounding these points in an \(\varepsilon\) radius contain at least 4 points (including the point itself). Because they are all reachable from one another, they form a single cluster. Points \(B\) and \(C\) are not core points, but are reachable from \(A\) (via other core points) and thus belong to the cluster as well. Point \(N\) is a noise point that is neither a core point nor directly-reachable. From <a href="https://en.wikipedia.org/wiki/DBSCAN#Advantages">DBSCAN Wikipedia page</a>.</p>

<p><br /></p>
<h3 id="pseudo-code">Pseudo code</h3>
<ul>
  <li>Associate all points to cluster -1 (outliers)</li>
  <li>Initialize the current cluster number \(C\) to 1</li>
  <li>For each point \(p\) in dataset:
    <ul>
      <li>\(N := \text{neighbours of } p\) (ie points at a distance from \(p\) less than \(\varepsilon\))</li>
      <li>if \(\vert N \vert \gt min_{points}\):
        <ul>
          <li>if \(cluster(p) = -1\):
            <ul>
              <li>\(cluster(p) := C\),</li>
              <li>\(C := C + 1\),</li>
            </ul>
          </li>
          <li>for each point \(p'\) in \(N\) (neighbours of \(p\)):
            <ul>
              <li>\(cluster(p') := cluster(p)\).</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>
<h1 id="pros-and-cons">Pros and cons</h1>
<h3 id="pros">Pros</h3>
<ul>
  <li>Can cluster different structure shape,</li>
  <li>Average complexity: \(O(nlog(n))\),</li>
  <li>Automatically selects the number of clusters.</li>
</ul>

<p><br /></p>
<h3 id="cons">Cons</h3>
<ul>
  <li>Sensitive to hyperparameters (particularly eps)</li>
</ul>

<p><br /></p>
<h1 id="kd-tree">KD tree</h1>
<p>KD tree is a method to avoid computing the whole distance matrix (ie to compute for each point its distance to every other point).
KD tree build a data structure that organises the data into a tree structure.</p>

<p><br /></p>
<h3 id="pseudo-code-1">Pseudo code</h3>
<ul>
  <li>Pick random feature x,</li>
  <li>Find the median to split the data set (half the data will have a value x greater than the median),</li>
  <li>Repeat until you have a predetermined number of points in each branch</li>
</ul>

<p><br /></p>
<h1 id="ressources">Ressources</h1>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/DBSCAN#Advantages">DBSCAN Wikipedia page</a>,</li>
  <li><a href="https://scikit-learn.org/stable/modules/clustering.html#dbscan">Sklearn Clustering page - DBSCAN</a>,</li>
  <li><a href="https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf">A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise by M Ester, H-P Kriegel, J Sander an X Xu</a>.</li>
</ul>

  </body>
</html>
