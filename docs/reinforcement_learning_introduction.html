<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Reinforcement Learning - Introduction</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="http://localhost:4000/Revision/">Revision</a></h1>
    <h2 id="back-to-reinforcement-learning"><a href="/Revision/reinforcement_learning.html">Back to Reinforcement Learning</a></h2>

<p><br /></p>
<h1 id="introduction">Introduction</h1>
<p>Reinforcement Learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward.</p>

<p>More formally the agent takes actions in the environment and these action lead him to win some rewards and lead him to new states with a new possible ensembles of actions.</p>

<p>Learning an agent to play a video game is an example of Reinforcement Learning. For example in Super Mario, a positive reward would be rewarded if the agent reach the end of the level and a negative reward would be given if the agent loose (fall in a hole, touche a turtle , etc.).</p>

<p>The environment is typically stated in the form of a Markov decision process.</p>

<p><br /></p>
<h1 id="notation">Notation</h1>
<h2 id="inputs">Inputs</h2>
<h3 id="markov-decision-process">Markov Decision Process</h3>
<p>A Markov Decision Process is defined by:</p>
<ul>
  <li>\(\mathcal{S}\): set of all states,</li>
  <li>\(\mathcal{A}\): set of all actions,</li>
  <li>\(\mathcal{A}(s)\): set of all actions available in state \(s\),</li>
  <li>\(\mathcal{R}\): set of all rewards,</li>
  <li>\(p(s',r \; \vert \; s,a)\): probability of next state \(s'\) and reward \(r\), given current state \(s\) and current action \(a\) (\(\mathbb{P}(S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a)\))</li>
  <li>\(\gamma\): discount rate (where \(0 \leq \gamma \leq 1\))</li>
</ul>

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/MDP1.png" width="50%" height="50%" />
</div>
<p><br /></p>

<p>At each time we have a state, action and reward:</p>
<ul>
  <li>\(S_t\): state at time \(t\),</li>
  <li>\(A_t\): action at time \(t\),</li>
  <li>\(R_t\): reward at time \(t\).</li>
</ul>

<p>From the future reward and \(\gamma\), we can define:</p>
<ul>
  <li>\(G_t\): discounted return at time t (\(\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)).</li>
</ul>

<p>The states \(\mathcal{S}\) and the possible actions \(\mathcal{A}\) are genrally known by the agent but not the rewards \(\mathcal{R}\) neither the dynamic \(p(s',r \; \vert \; s,a)\) of the environment.</p>

<p><br /></p>
<h2 id="outputs">Outputs</h2>
<h3 id="policy">Policy</h3>
<p>The agent will follow some strategies (policies) to deal with the environment.</p>

<ul>
  <li>\(\pi\): a policy
    <ul>
      <li>if the environment is deterministic: \(\pi(s) \in \mathcal{A}(s) \forall s \in \mathcal{S}\),</li>
      <li>if the environment is stochastic: \(\pi(a \vert s) = \mathbb{P}(A_t=a \vert S_t=s) \forall s \in \mathcal{S} \text{ and } \forall a \in \mathcal{A}(s)\).</li>
    </ul>
  </li>
</ul>

<p>In a deterministic environment, the policy for an agent is an action for each state.</p>

<p>In a stochastic environment it is a probability distribution of actions for each state.</p>

<h5 id="deterministic-policy">Deterministic policy</h5>
<p>A deterministic policy associates a single action \(a\) to a state \(s\). That means that in state \(s\) the same action \(a\) will always be choosen.</p>

<h5 id="stochastic-policy">Stochastic policy</h5>
<p>A stochastic policy, on the other hand, associate to each state \(s\) a probability for every possible action \(a\).</p>

<p><br /></p>
<h3 id="state-value-function">State-value function</h3>
<p>The state-value function \(v_{\pi}\) for a given policy \(\pi\) returns an extpected value of rewards for each state \(s\):</p>

\[v_{\pi}(s) = \mathbb{E} [G_t \vert S_t=s] \forall s \in \mathcal{S}\]

<p><br /></p>

<p>The optimal state-value function \(v_{*}\) is the state-value function associated with the policy \(\pi\) that maximises the reward for each state \(s\):</p>

\[v_{*}(s) = \max_{\pi} v_{\pi}(s) \forall s \in \mathcal{S}\]

<p><br /></p>
<h3 id="action-value-function">Action-value function</h3>
<p>The action-value function \(q_{\pi}\) for a given policy \(\pi\) returns an extpected value of rewards for each state \(s\) and each \(action\):</p>

\[q_{\pi}(s,a) = \mathbb{E} [G_t \vert S_t=s, A_t=a] \forall s \in \mathcal{S} \text{ and } \forall a \in \mathcal{A}(s)\]

<p><br /></p>

<p>The optimal action-value function \(q_{*}\) is the action-value function associated with the policy \(\pi\) that maximises the reward for each state \(s\) and action \(a\):</p>

\[q_{*}(s, a) = \max_{\pi} q_{\pi}(s, a) \forall s \in \mathcal{S} \text{ and } \forall a \in \mathcal{A}(s)\]

<h3 id="summary">Summary</h3>
<p><br /></p>
<div style="text-align: center">
<img src="assets/images/MDP2.png" width="50%" height="50%" />
</div>
<p><br /></p>

<p><br /></p>
<h2 id="resources">Resources</h2>
<p>See:</p>
<ul>
  <li>Udacity Deep Reinforcement Learning nanodegree (UDRLN) cheat sheet part 1 and 2,</li>
  <li>UDRLN video 1.2.11.</li>
</ul>

<p><br /></p>
<h1 id="bellman-equations">Bellman equations</h1>
<p>Bellman equations are a set of dynamic programing equation that expresses the state-value and action-value functions of a state \(s\) (and action \(a\)) using next states and actions.</p>

<p><br /></p>
<h2 id="state-value-equations">State-value equations</h2>
<p>The state-value function \(v_{\pi}\) for a state \(s\) can be expressed as:</p>

\[v_{\pi}(s) = \sum_{a \in \mathcal{A}(s)} \pi (a \vert s) \sum_{s' \in \mathcal{S}, r \in \mathcal{R}}p(s',r  \; \vert \; s,a)(r + \gamma v_{\pi}(s'))\]

<p><br /></p>
<div style="text-align: center">
<img src="assets/images/bellman1.png" width="30%" height="30%" />
</div>
<p><br /></p>

<p>The Bellman optimal equation for value function is:</p>

\[v_{*}(s) = \max_{a \in \mathcal{A}(s)} \sum_{s' \in \mathcal{S}, r \in \mathcal{R}}p(s',r  \; \vert \; s,a)(r + \gamma v_{*}(s'))\]

<p>The optimal state-value function \(v_{*}\) is defined as an application of its definition: it is the reward associated with the action that maximises this reward.</p>

<p><br /></p>
<h2 id="action-value-equations">Action-value equations</h2>
<p>The action-value function \(q_{\pi}\) for a state \(s\) and action \(a\) can be expressed as:</p>

\[q_{\pi}(s,a) = \sum_{s' \in \mathcal{S}, r \in \mathcal{R}}p(s',r  \; \vert \; s,a) \left(r + \gamma \sum_{a' \in \mathcal{A}(s')} \pi (a' \vert s') q_{\pi}(s',a') \right)\]

<p><br /></p>

<p>The Bellman optimal equation for action function is:</p>

\[q_{\pi}(s,a) = \sum_{s' \in \mathcal{S}, r \in \mathcal{R}}p(s',r  \; \vert \; s,a) \left(r + \gamma \max_{a' \in \mathcal{A}(s')}\right)\]

<p><br /></p>
<h2 id="resources-1">Resources</h2>
<p>See:</p>
<ul>
  <li>UDRLN cheat sheet part 3,</li>
  <li>UDRLN video 1.3.5</li>
</ul>

  </body>
</html>
