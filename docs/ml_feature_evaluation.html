<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>ML System Design - Feature Evaluation</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="/Revision/">Revision</a></h1>
    <h2 id="back-to-ml-system-design"><a href="/Revision/system_design.html">Back to ML System Design</a></h2>

<p><br /></p>
<h1 id="feature-evaluation">Feature Evaluation</h1>
<ol>
  <li>Feature importance</li>
  <li>Feature generalization</li>
</ol>

<p><br /></p>
<h2 id="feature-importance">Feature importance</h2>
<h3 id="model-specific">Model specific</h3>
<p>Some models like tree models can output features importance. For example XGBoost:
<br /></p>
<div style="text-align: center">
<img src="assets/images/xg_boost_fi.png" width="40%" height="40%" />
</div>

<p><br /></p>
<h3 id="model-agnostic">Model agnostic</h3>
<h4 id="shap-shapley-additive-explanations">SHAP: SHapley Additive exPlanations</h4>
<p>SHAP can be applied to measure a feature’s contribution to a single prediction:
<br /></p>
<div style="text-align: center">
<img src="assets/images/shap1.png" width="40%" height="40%" />
</div>

<p>Or to measure a feature’s contribution to the entire model:
<br /></p>
<div style="text-align: center">
<img src="assets/images/shap2.png" width="40%" height="40%" />
</div>

<p><br /></p>
<h1 id="resources">Resources</h1>
<p>See:</p>
<ul>
  <li><a href="https://christophm.github.io/interpretable-ml-book/lime.html">Course on ML Interpretability</a>,</li>
  <li><a href="https://towardsdatascience.com/shap-a-reliable-way-to-analyze-your-model-interpretability-874294d30af6">This Toward Data Science post</a>.</li>
</ul>

  </body>
</html>
