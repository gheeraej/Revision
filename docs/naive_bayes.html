<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Naive Bayes</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="/Revision/">Revision</a></h1>
    <h2 id="back-to-machine-learning"><a href="/Revision/machine_learning.html">Back to Machine Learning</a></h2>

<p><br /></p>
<h1 id="introduction">Introduction</h1>
<p>Naive Bayes is a classifier algorithm based on the Bayes’ theorem with strong (or even naive) assumptions.
The model takes data \(X\) as input (generally discrete) and outputs a discrete value \(Y\).</p>

<p><br /></p>
<h1 id="bayes-theorem">Bayes’ theorem</h1>
<p>The <a href="/Revision/mathematics_bayesian_statistics.html#bayes_theorem">Baye’s theorem</a> is:</p>

\[p(y \vert x) = \frac{p(x \vert y)p(y)}{p(x)}\]

<p>It is just a derivation of this:</p>

\[p(y \vert x) = p(x \cap y) = p(x \vert y)p(y)\]

<p>Another formulation is:</p>

\[posterior = \frac{prior \times likelihood}{evidence}\]

<p><br /></p>
<h1 id="assumptions">Assumptions</h1>
<p>The Naive Bayes model makes the assumption that the features are independant given the output.</p>

<p>Let’s take the same example as <a href="https://cs229.stanford.edu/notes2021fall/cs229-notes2.pdf">Andrew Ng CS229 course</a> to explain that. In a spam email classification, if one knows that an email is a spam (\(Y=1\)) then knowing if the words ‘buy’ appears in the email won’t impact its belief about the appearance of the word ‘price’ in the email.</p>

<p>The normal behaviour would be that if the word ‘buy’ appears in the email the probability that the word ‘price’ also appears would be higher. That’s why the assumption is said to be naive.</p>

<p>More formally the naive bayes assumption is for two features \(X_1\) and \(X_2\):</p>

\[\begin{eqnarray}
p(X_1, X_2 \vert Y) &amp;&amp;= p(X_1 \vert Y) p(X_2 \vert Y, X_1) \\
\text{Using Naive Bayes assumption }\Rightarrow &amp;&amp;= p(X_1 \vert Y) p(X_2 \vert Y)
\end{eqnarray}\]

<p>And for \(d\) features \(X_1, \ldots, X_d\) we get:</p>

\[\begin{eqnarray}
p(X_1, \ldots, X_d \vert Y) &amp;&amp;= p(X_1 \vert Y) p(X_2 \vert Y, X_1) \ldots  p(X_d \vert Y, X_{d-1}, \ldots, X_1)\\
\text{Using Naive Bayes assumption }\Rightarrow &amp;&amp;= p(X_1 \vert Y) p(X_2 \vert Y) \ldots p(X_d \vert Y) \\
&amp;&amp;= \prod_{i=1}^d p(X_i \vert Y)
\end{eqnarray}\]

<p>Equivalently we have:
\(\begin{eqnarray}
p(X_1, \ldots, X_d, Y) &amp;&amp;= p(Y) p(X_1 \vert Y) p(X_2 \vert Y, X_1) \ldots  p(X_d \vert Y, X_{d-1}, \ldots, X_1)\\
\text{Using Naive Bayes assumption }\Rightarrow &amp;&amp;= p(Y) p(X_1 \vert Y) p(X_2 \vert Y) \ldots p(X_d \vert Y) \\
&amp;&amp;= p(Y) \prod_{i=1}^d p(X_i \vert Y)
\end{eqnarray}\)</p>

<p>Using the Bayes theorm we know that \(p(Y \vert X) = \frac{p(X \vert Y)p(Y)}{p(X)} = \frac{p(X,Y)}{p(X)}\). When the \(X\) are known, the denominator is just a constant and thus:</p>

\[\begin{eqnarray}
p(Y \vert X_1, \ldots, X_d) &amp;&amp; \varpropto p(X_1, \ldots, X_d \vert Y)p(Y) \\
&amp;&amp; \varpropto p(Y) \prod_{i=1}^d p(X_i \vert Y)
\end{eqnarray}\]

<p><br /></p>
<h1 id="formula">Formula</h1>
<p>We will express the formulas using the notation \(k\) for the class of \(Y\). In the binary case, \(k \in \{0, 1\}\) and in the multiclass case \(k \in \{1, \ldots, K\}\).</p>

<h2 id="bernouilli-naive-bayes">Bernouilli Naive Bayes</h2>
<p>In the Bernouilli Naive Bayes, the input \(X\) of the model are Bernoulli variables that tells us if the variable \(X_i\) is present or not.</p>

<p>Hence each \(X_i \sim \mathcal{B}(\phi_{i, k})\) for \(Y\) being of class \(k\).</p>

\[p(X \vert Y=k) = \prod_{i=1}^n \phi_{i, k}^{X_i} (1 - \phi_{i, k})^{(1-X_i)}\]

<p><br /></p>
<h2 id="multinomial-naive-bayes">Multinomial Naive Bayes</h2>
<p>In the Multinomial Naive Bayes, the input \(X_i\) of the model events have been generated by multinomial distributions with different parameters \(M\) being the number of possible outcomes and \(p_1, \ldots, p_M\) where \(p_m\) is the probability that event \(m\) occurs.</p>

<p>Each \(X_i\) can be view as one of the columns of an histogram of a multinomial distribution.</p>

\[\begin{eqnarray}
X &amp;&amp;= \left(X_{1, 1}, \ldots, X_{1, M_1}, X_{2, 1}, \ldots, X_{2, M_2}, \ldots, X_{d, 1}, \ldots, X_{d, M_d}\right)
&amp;&amp;= \left(X_1, X_n\right)
\end{eqnarray}\]

<p>Where:</p>
<ul>
  <li>\(d\) is the number of multinomial distributions,</li>
  <li>\(M_d\) is the number of possible outcomes for multinomial distribution \(d\),</li>
  <li>\(n = \sum_{i=1}^d M_d\).</li>
</ul>

<p>And \(Y \sim \mathcal{B}(\phi_Y)\).</p>

\[p(X \vert Y=k) = \frac{(\sum_{i=1}^n X_{i, k})!} {\prod_{i=1}^n X_{i, k}!} \prod_{i=1}^n {p_{i, k}}^{X_{i, k}}\]

<p><br /></p>
<h2 id="gaussian-naive-bayes">Gaussian Naive Bayes</h2>
<p>In the Multinomial Naive Bayes, the input \(X\) of the model events have been generated by a gaussian distribution.</p>

<p>Hence the input data \(X \sim \mathcal{N}(\mu_k, \sigma_k^2)\) where \(\mu_k \in \mathbb{R}^{n_{pop}}\) is a vector and \(\sigma_k^2 \in \mathbb{R}^{n_{pop} \times n_{pop}}\) is a diagonal matrix and:</p>

\[p(X=x \vert Y=k)=\frac{1}{\sqrt{2 \pi \sigma_k^2}} \exp \left(- \frac{(x - \mu_k)^2}{2 \sigma_k^2}\right)\]

<p><br /></p>
<h1 id="calibration">Calibration</h1>
<p>For both type of naive bayes the calibration is done using MLE:</p>

\[\begin{eqnarray}
L(\phi_y, \phi_{i,Y=0}, \phi_{i,Y=1}) &amp;&amp;= \prod_{j=1}^{n_{pop}} p(X^{(j)}, Y^{(j)}) \\
&amp;&amp;= \prod_{j=1}^{n_{pop}} p(X^{(j)} \vert Y^{(j)}) p(Y^{(j)})
\end{eqnarray}\]

<p>Applying the \(\log\) (monotone strictly increasing function) we get:</p>

\[\begin{eqnarray}
l(\phi_y, \phi_{i,Y=0}, \phi_{i,Y=1}) &amp;&amp;= \log L(\phi_y, \phi_{i,y=0}, \phi_{i,y=1}) \\
&amp;&amp;= \log \prod_{j=1}^{n_{pop}} p(X^{(j)} \vert Y^{(j)}) p(Y^{(j)}) \\
&amp;&amp;= \sum_{j=1}^{n_{pop}} \log \left[p(X^{(j)} \vert Y^{(j)}) p(Y^{(j)}) \right]
\end{eqnarray}\]

<p><br /></p>
<h2 id="bernouilli-naive-bayes-1">Bernouilli Naive Bayes</h2>
<p>Now replacing \(p(X^{(j)} \vert Y^{(j)})\) by their distribution we get:</p>

\[\begin{eqnarray}
l(\phi_y, \phi_{i,Y=0}, \phi_{i,Y=1}) &amp;&amp;= \sum_{j=1}^{n_{pop}} \log \left(p(X^{(j)} \vert Y^{(j)}) p(Y^{(j)}) \right) \\
&amp;&amp;= \sum_{j=1}^{n_{pop}} \log \prod_{i=1}^n \phi_i^{X_i^{(j)}} (1 - \phi_i)^{1-X_i^{(j)}} (\phi_Y)^Y (1-\phi_Y)^{1-Y}\\
&amp;&amp;= \sum_{j=1}^{n_{pop}} \sum_{i=1}^n \left[X_i^{(j)} \log \phi_i + (1-X_i^{(j)}) \log (1 - \phi_i)\right] + Y \log \phi_Y + (1-Y) \log (1-\phi_Y)
\end{eqnarray}\]

<p>Setting the derivatives to 0 wrt to each parameters we obtain:</p>

\[\phi_{i,Y=0} = \frac{\sum_{j=1}^{n_{pop}} \mathbf{1}_{X_i^{(j)}=0, Y^{(j)}=0}} {\sum_{j=1}^{n_{pop}} \mathbf{1}_{Y^{(j)}=0}}\]

\[\phi_{i,Y=1} = \frac{\sum_{j=1}^{n_{pop}} \mathbf{1}_{X_i^{(j)}=1, Y^{(j)}=1}} {\sum_{j=1}^{n_{pop}} \mathbf{1}_{Y^{(j)}=1}}\]

\[\phi_Y = \frac{\sum_{j=1}^{n_{pop}} \mathbf{1}_{Y^{(j)}=1}}{n_{pop}}\]

<p><br /></p>
<h2 id="multinomial-naive-bayes-1">Multinomial Naive Bayes</h2>
<p>Same steps can be applied to find parameters.</p>

<p><br /></p>
<h2 id="gaussian-naive-bayes-1">Gaussian Naive Bayes</h2>
<p>Same steps can be applied to find parameters.</p>

<p><br /></p>
<h1 id="resources">Resources</h1>
<p>See:</p>
<ul>
  <li><a href="https://cs229.stanford.edu/notes2021fall/cs229-notes2.pdf">CS229</a>.</li>
</ul>

  </body>
</html>
