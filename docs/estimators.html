<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Estimators</title>
    <link rel="stylesheet" href="assets/css/main.css">
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  </head>
  <body>
    <h1><a href="http://localhost:4000/Revision/">Revision</a></h1>
    <h2 id="back-to-machine-learning"><a href="/Revision/machine_learning.html">Back to Machine Learning</a></h2>

<p><br /></p>
<h1 id="ordinary-least-squares">Ordinary Least Squares</h1>
<p>See <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Wikipedia page of Ordinary Least Squares</a>.</p>

<p><br /></p>
<h3 id="link-with-least-squares">Link with Least Squares</h3>
<p>Ordinary Least Squares is a special case of Least Squares. OLS is the most commonly used estimator of the LS family. To have more insights on the difference see:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Least_squares">Wikipedia page of Least Squares</a>,</li>
  <li><a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Wikipedia page of Ordinary Least Squares</a>,</li>
  <li><a href="https://stackoverflow.com/questions/67694355/difference-between-least-squares-ls-and-ordinary-least-squares-ols">This StackOverflow page</a>,</li>
  <li><a href="https://math.stackexchange.com/questions/1461776/what-is-the-difference-between-linear-least-squares-and-ordinary-least-squares">This StackExchange page</a>,</li>
  <li><a href="https://fr.scribd.com/document/14819165/Regressions-coniques-quadriques-circulaire-spherique">This document by JJ Jacquelin (in french)</a>.</li>
</ul>

<p><br /></p>
<h1 id="maximum-likelihood">Maximum Likelihood</h1>
<p>Given a sample of data \((X, Y)\) (or simply \(Y\) if we are not in a prediction framework using features), the goal of maximum likelihood estimation is to make inferences about the population that is most likely to have generated the sample, specifically the joint probability distribution of the random variables.</p>

<p>The goal of MLE is to find the best set of parameters \(\theta\) of a probability distribution function to match the empirical distribution of the value \(Y\) given the features \(X\). Hence the goal is to find the \(\theta\) that maximise the probability density function for the values we get.</p>

<p><br /></p>
<h2 id="resources">Resources</h2>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Wikipedia page of Maximum Likelihood Estimation</a>.</li>
</ul>

<p><br /></p>
<h1 id="maximum-a-posteriori">Maximum a Posteriori</h1>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">Wikipedia page of Maximum a Posteriori</a>.</li>
</ul>

<p><br /></p>
<h1 id="method-of-moments">Method of moments</h1>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">Wikipedia page of Method of moments</a>.</li>
</ul>

<p><br /></p>
<h1 id="bayes-estimator">Bayes estimator</h1>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Bayes_estimator">Wikipedia page of Bayes estimator</a>.</li>
</ul>

<p><br /></p>
<h1 id="other-estimators">Other estimators</h1>
<p>See:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Estimation_theory#Estimators">This list of estimators on Wikipedia</a>.</li>
</ul>

  </body>
</html>
